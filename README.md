# TranslationalSelfAttention

In this project, I am replacing the convolution operation with a self-attention operation applied to each convolutional window in the same way convolutional filters would be.  The intuition is that self-attention is a more powerful operation which not only subsumes convolution but also allows for dynamic kernel generation and content-based lookup
